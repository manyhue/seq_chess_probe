{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "## allow interaction with the plots\n",
    "%matplotlib widget\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# add the main directory reference and import 'imports.py'\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "from imports import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.WARN)\n",
    "\n",
    "__builtins__.verbosity = 4\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ[\"HF_HOME\"] = \"/run/media/HUNK/DATASETS/HF\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Declare a few global data/train params\n",
    "from lib.optim import WarmupCosineLR\n",
    "from typing import override\n",
    "from lib.train import *\n",
    "\n",
    "\n",
    "class params:\n",
    "    num_workers = 0\n",
    "    batch_size = 160\n",
    "    seq_len = 140\n",
    "    files_per_epoch = 800\n",
    "    prefetch_factor = (10,)\n",
    "\n",
    "    max_epochs = 200\n",
    "    lr = 0.005\n",
    "\n",
    "    epoch_every = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loader 0 (IterableDataset) Preview:\n",
      "--------------------------------------------------\n",
      "Constituent shapes:\n",
      "batch[0]: torch.Size([160, 140]), torch.int64\n",
      "batch[1]: torch.Size([160, 1]), torch.int64\n",
      "\n",
      "First 5 samples:\n",
      "\n",
      "Sample 0: \n",
      "\n",
      "tensor([1435, 5328, 1440, 6162, 1010, 6078, 1404, 5298, 1256, 5189,  363, 4097,\n",
      "        1527, 4368,  777, 5360, 1497, 5139, 1529, 4436,  158, 4331, 1064, 4156,\n",
      "        1170, 4086, 1076, 5541, 1042, 5949, 2089, 4366, 1373, 5443, 2122, 6106,\n",
      "        1410, 5295, 1380, 5699,  129, 5355, 1214, 4301,  294, 4942, 3567,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "\n",
      "0\n",
      "\n",
      "Sample 1: \n",
      "\n",
      "tensor([1435, 5328, 1440, 6162, 1010, 6078, 1404, 5298, 1256, 5189,  363, 4097,\n",
      "        1527, 4368,  777, 5360, 1497, 5139, 1529, 4436,  158, 4331, 1064, 4156,\n",
      "        1170, 4086, 1076, 5541, 1042, 5949, 2089, 4366, 1373, 5443, 2122, 6106,\n",
      "        1410,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "\n",
      "0\n",
      "\n",
      "Sample 2: \n",
      "\n",
      "tensor([1435, 5328, 1440, 6162, 1010, 6078, 1404, 5298, 1256, 5189,  363, 4097,\n",
      "        1527, 4368,  777, 5360, 1497, 5139, 1529, 4436,  158, 4331, 1064,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "\n",
      "3\n",
      "\n",
      "Sample 3: \n",
      "\n",
      "tensor([1435, 5328, 1440, 6162, 1010, 6078, 1404, 5298, 1256, 5189,  363, 4097,\n",
      "        1527, 4368,  777, 5360, 1497, 5139, 1529, 4436,  158, 4331, 1064, 4156,\n",
      "        1170, 4086, 1076, 5541, 1042, 5949, 2089, 4366, 1373, 5443, 2122, 6106,\n",
      "        1410, 5295, 1380,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "\n",
      "0\n",
      "\n",
      "Sample 4: \n",
      "\n",
      "tensor([1435, 5328, 1440, 6162, 1010, 6078, 1404, 5298, 1256, 5189,  363, 4097,\n",
      "        1527, 4368,  777, 5360, 1497, 5139, 1529, 4436,  158, 4331, 1064, 4156,\n",
      "        1170, 4086, 1076, 5541, 1042, 5949, 2089, 4366, 1373, 5443, 2122, 6106,\n",
      "        1410, 5295, 1380, 5699,  129, 5355, 1214, 4301,  294, 4942, 3567, 4727,\n",
      "         381, 7009,  392, 4786,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0])\n",
      "\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from data.pgnSquare import *\n",
    "\n",
    "\n",
    "dt = PGNSquareData(PGNSquareDataConfig.create(params))\n",
    "\n",
    "dt.preview()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume `model` is your pretrained transformer model\n",
    "# Freeze the entire model\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# The layer you want to hook into:\n",
    "target_layer = model.encoder.layers[-1]  # example, adjust as needed\n",
    "\n",
    "# Add a linear layer for classification\n",
    "fc = nn.Linear(hidden_dim, num_classes).to(device)\n",
    "\n",
    "# Forward hook to capture the output of the target layer\n",
    "activations = None\n",
    "\n",
    "\n",
    "def hook_fn(m, i, o):\n",
    "    nonlocal activations\n",
    "    activations = o.detach()\n",
    "\n",
    "\n",
    "hook_handle = target_layer.register_forward_hook(hook_fn)\n",
    "\n",
    "# Optimizer only for fc layer\n",
    "optimizer = torch.optim.Adam(fc.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "model.eval()  # model is frozen, so eval mode is fine if it matters\n",
    "fc.train()\n",
    "\n",
    "for batch in train_loader:\n",
    "    inputs, labels = batch  # adjust based on your data structure\n",
    "    optimizer.zero_grad()\n",
    "    # Forward pass through the model to fill `activations` from the hook\n",
    "    with torch.no_grad():\n",
    "        _ = model(inputs.to(device))\n",
    "\n",
    "    # Now forward pass through the fc layer\n",
    "    outputs = fc(activations)\n",
    "    loss = loss_fn(outputs, labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
