{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "\n",
    "## allow interaction with the plots\n",
    "%matplotlib widget\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# add the main directory reference and import 'imports.py'\n",
    "import sys\n",
    "import os\n",
    "\n",
    "if \"..\" not in sys.path:\n",
    "    sys.path.insert(0, \"..\")\n",
    "from imports import *\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(level=logging.WARN)\n",
    "\n",
    "__builtins__.verbosity = 4\n",
    "# Set the HF_HOME environment variable\n",
    "os.environ[\"HF_HOME\"] = \"/run/media/HUNK/DATASETS/HF\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class params:\n",
    "    num_workers = 24\n",
    "    # batch_size = 160\n",
    "    batch_size = 64\n",
    "    seq_len = 140\n",
    "    files_per_epoch = 800\n",
    "\n",
    "    max_epochs = 5\n",
    "    # lr = 0.005\n",
    "    lr = 0.005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 91.13M\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/ARCHIVE/Personal/2186474940/lib/modules.py:75: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(pth_path)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 91.04M\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(7797, 768)\n",
       "    (wpe): Embedding(140, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=7797, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load models\n",
    "\n",
    "from lib.chess import chess_move_labels as le\n",
    "from models.nanoRWKV import *\n",
    "from models.nanoGPT import *\n",
    "\n",
    "rwkv_model = RWKV(GPTConfig(vocab_size=len(le.classes_), bias=True, seq_len=140))\n",
    "\n",
    "rwkv_model.load_from_pth(\n",
    "    \"out/RWKV__seq_len=140__vocab_size=7797__n_layer=12__n_head=12__n_embd=768__dropout=0.0__bias=True__ignore_index=0__lr=0.005__weight_decay=0.01__epoch=164-239.pth\"\n",
    ")\n",
    "\n",
    "rwkv_model.to(device)\n",
    "\n",
    "gpt_model = GPT(GPTConfig(vocab_size=len(le.classes_), bias=True, seq_len=140))\n",
    "\n",
    "gpt_model.load_from_pth(\n",
    "    \"out/GPT__seq_len=140__vocab_size=7797__n_layer=12__n_head=12__n_embd=768__dropout=0.0__bias=True__lr=0.01__weight_decay=0.01__epoch=133-226.pth\"\n",
    ")\n",
    "gpt_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 games to games_1.pgn\n",
      "Saved 100 games to games_2.pgn\n",
      "Saved 100 games to games_3.pgn\n",
      "Saved 100 games to games_4.pgn\n",
      "Saved 100 games to games_5.pgn\n",
      "Saved 100 games to games_6.pgn\n",
      "Saved 100 games to games_7.pgn\n",
      "Saved 100 games to games_8.pgn\n",
      "Saved 100 games to games_9.pgn\n",
      "Saved 100 games to games_10.pgn\n",
      "Saved 100 games to games_11.pgn\n",
      "Saved 100 games to games_12.pgn\n",
      "Saved 100 games to games_13.pgn\n",
      "Saved 100 games to games_14.pgn\n",
      "Saved 100 games to games_15.pgn\n",
      "Saved 100 games to games_16.pgn\n",
      "Saved 100 games to games_17.pgn\n",
      "Saved 100 games to games_18.pgn\n",
      "Saved 100 games to games_19.pgn\n",
      "Saved 100 games to games_20.pgn\n",
      "Saved 100 games to games_21.pgn\n",
      "Saved 100 games to games_22.pgn\n",
      "Saved 100 games to games_23.pgn\n",
      "Saved 100 games to games_24.pgn\n",
      "Saved 100 games to games_25.pgn\n",
      "Saved 100 games to games_26.pgn\n",
      "Saved 100 games to games_27.pgn\n",
      "Saved 100 games to games_28.pgn\n",
      "Saved 100 games to games_29.pgn\n",
      "Saved 100 games to games_30.pgn\n",
      "Saved 100 games to games_31.pgn\n",
      "Saved 100 games to games_32.pgn\n",
      "Saved 100 games to games_33.pgn\n",
      "Saved 100 games to games_34.pgn\n",
      "Saved 100 games to games_35.pgn\n",
      "Saved 100 games to games_36.pgn\n",
      "Saved 100 games to games_37.pgn\n",
      "Saved 100 games to games_38.pgn\n",
      "Saved 100 games to games_39.pgn\n",
      "Saved 100 games to games_40.pgn\n",
      "Saved 100 games to games_41.pgn\n",
      "Saved 100 games to games_42.pgn\n",
      "Saved 100 games to games_43.pgn\n",
      "Saved 100 games to games_44.pgn\n",
      "Saved 100 games to games_45.pgn\n",
      "Saved 100 games to games_46.pgn\n",
      "Saved 100 games to games_47.pgn\n",
      "Saved 100 games to games_48.pgn\n",
      "Saved 100 games to games_49.pgn\n",
      "Saved 100 games to games_50.pgn\n"
     ]
    }
   ],
   "source": [
    "# synthetic data\n",
    "\n",
    "from lib.chess import generate_random_games_to_pgn\n",
    "\n",
    "# with change_dir(\"resources/synthetic\"):\n",
    "#     generate_random_games_to_pgn(500, game_length=params.seq_len + 1)\n",
    "\n",
    "\n",
    "with change_dir(\"resources/synthetic_val\"):\n",
    "    generate_random_games_to_pgn(50, game_length=params.seq_len + 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data.pgnSeq import *\n",
    "\n",
    "synth_dt = PGNData(\n",
    "    PGNDataConfig.create(\n",
    "        params, directory=\"resources/synthetic_val\", files_per_epoch=20\n",
    "    )\n",
    ")\n",
    "\n",
    "# synth_dt.preview(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepatory function defs\n",
    "\n",
    "# customize trainer\n",
    "from extras.train import SeqTrainer\n",
    "from lib.optim import WarmupCosineLR\n",
    "\n",
    "\n",
    "def eval_legal_movemaking(model, data):\n",
    "    trainer = SeqTrainer(\n",
    "        TrainerConfig.create(\n",
    "            params,\n",
    "        )\n",
    "    )\n",
    "    trainer.init(model, data.loaders())\n",
    "\n",
    "    evaluator = [[], []]\n",
    "\n",
    "    def batch_fun(outputs, batch, batch_num):\n",
    "        for pred_seq, input_seq in zip(map(model.pred, outputs), batch[0]):\n",
    "            try:\n",
    "                original = data.le.inverse_transform(input_seq.flatten().cpu())\n",
    "                evaluator[0].append(original)\n",
    "\n",
    "                decoded = data.le.inverse_transform(pred_seq.flatten().cpu())\n",
    "                evaluator[1].append(decoded)\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "    loaders = data.loaders()\n",
    "    loader = loaders[1] if len(loaders) > 1 else loaders[0]\n",
    "\n",
    "    trainer.eval(loader=loader, batch_fun=batch_fun)\n",
    "\n",
    "    total = 0\n",
    "    legal = 0\n",
    "\n",
    "    for game, preds in zip(*evaluator):\n",
    "        board = chess.Board()\n",
    "        for move, pred in zip(game, preds):\n",
    "            if move == \"<PAD>\":\n",
    "                break\n",
    "            else:\n",
    "                board.push_uci(move[1:])\n",
    "                if pred == \"<PAD>\":\n",
    "                    continue\n",
    "                total += 1\n",
    "                if chess.Move.from_uci(pred[1:]) in list(board.legal_moves):\n",
    "                    legal += 1\n",
    "\n",
    "    print(legal, total, legal / total * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "184217 272571 67.5849595151355\n",
      "19468 69022 28.205499695749182\n"
     ]
    }
   ],
   "source": [
    "eval_legal_movemaking(rwkv_model, synth_dt)\n",
    "eval_legal_movemaking(gpt_model, synth_dt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loader 0 (IterableDataset) Preview:\n",
      "--------------------------------------------------\n",
      "Constituent shapes:\n",
      "batch[0]: torch.Size([64, 141]), torch.int64\n",
      "\n",
      "First 1 samples:\n",
      "\n",
      "Sample 0: \n",
      "\n",
      "tensor([1435, 5328, 1440, 6162, 1010, 6078, 1404, 5298, 1256, 5189,  363, 4097,\n",
      "        1527, 4368,  777, 5360, 1497, 5139, 1529, 4436,  158, 4331, 1064, 4156,\n",
      "        1170, 4086, 1076, 5541, 1042, 5949, 2089, 4366, 1373, 5443, 2122, 6106,\n",
      "        1410, 5295, 1380, 5699,  129, 5355, 1214, 4301,  294, 4942, 3567, 4727,\n",
      "         381, 7009,  392, 4786, 3461, 4168, 3013, 4843, 3460,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0])\n",
      "\n",
      "Loader 1 (IterableDataset) Preview:\n",
      "--------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/multiprocessing/util.py\", line 136, in _remove_temp_dir\n",
      "    rmtree(tempdir, onerror=onerror)\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 759, in rmtree\n",
      "    _rmtree_safe_fd(stack, onexc)\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 703, in _rmtree_safe_fd\n",
      "    onexc(func, path, err)\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 750, in onexc\n",
      "    return onerror(func, path, exc_info)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 662, in _rmtree_safe_fd\n",
      "    os.rmdir(name, dir_fd=dirfd)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-mset5oco'\n",
      "Traceback (most recent call last):\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/multiprocessing/util.py\", line 303, in _run_finalizers\n",
      "    finalizer()\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/multiprocessing/util.py\", line 227, in __call__\n",
      "    res = self._callback(*self._args, **self._kwargs)\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/multiprocessing/util.py\", line 136, in _remove_temp_dir\n",
      "    rmtree(tempdir, onerror=onerror)\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 759, in rmtree\n",
      "    _rmtree_safe_fd(stack, onexc)\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 703, in _rmtree_safe_fd\n",
      "    onexc(func, path, err)\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 750, in onexc\n",
      "    return onerror(func, path, exc_info)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/ARCHIVE/Personal/2186474940/.pixi/envs/dev/lib/python3.12/shutil.py\", line 662, in _rmtree_safe_fd\n",
      "    os.rmdir(name, dir_fd=dirfd)\n",
      "OSError: [Errno 39] Directory not empty: '/tmp/pymp-xbme6uor'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Constituent shapes:\n",
      "batch[0]: torch.Size([4, 141]), torch.int64\n",
      "\n",
      "First 1 samples:\n",
      "\n",
      "Sample 0: \n",
      "\n",
      "tensor([1373, 5297, 1010, 4943, 1496, 5422,  366, 4333, 1435, 5360, 1255, 5188,\n",
      "         777, 4728, 1403, 5236,  128, 4983, 2089, 7563,  307, 4395, 1157, 5292,\n",
      "        1063, 7006, 1465, 5266, 1341, 5328, 1440, 5355, 1379, 5094,  545, 5391,\n",
      "         476, 7118, 1382, 7107, 1385, 6164, 3567, 5418,  452, 5414, 1157, 5127,\n",
      "         520, 5021,  890, 5111,  270, 5382, 1526, 7435, 3011, 7211, 3453, 4097,\n",
      "         356, 5387,  178, 4842, 3229, 5001,  249, 4835, 3247, 4827, 3297, 4455,\n",
      "         842, 5260, 3082, 4132, 3416, 4908, 3395, 5443, 1320, 4955,  383, 4884,\n",
      "        3066, 4946, 3175, 4919,  314, 5439, 1501, 4821, 1324, 5384, 1326, 5381,\n",
      "         249, 5021, 1407, 4200, 3166, 5112, 1328, 4365,  895, 4275,  838, 5018,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "           0,    0,    0,    0,    0,    0,    0,    0,    0])\n"
     ]
    }
   ],
   "source": [
    "elite_dt = PGNData(\n",
    "    PGNDataConfig.create(\n",
    "        params,\n",
    "        directory=\"resources/lichess_elite\",  # irrelevant\n",
    "        val_directory=\"resources/lichess_elite_val\",\n",
    "        files_per_epoch=None,\n",
    "        max_games_per_file=999,\n",
    "    )\n",
    ")\n",
    "\n",
    "elite_dt.preview(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424015 615250 68.91751320601381\n",
      "109593 275547 39.772888109832444\n"
     ]
    }
   ],
   "source": [
    "eval_legal_movemaking(rwkv_model, elite_dt)\n",
    "eval_legal_movemaking(gpt_model, elite_dt)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
